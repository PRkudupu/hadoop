PORT FOR HDFS
 50070
DIFFERENT NODES
	Name Node 
	Secondary Node
	Data Node
	JPS ->Standard Java Utility
	
NODE MANAGER
RESOURCE MANAGER
	Master node 
	Name node is present
	
PORT 8088 CLUSTER
	Web interface for the cluster
HDFS
	Built on commondity hardware
	Highly fault tolerant hardware failure is the norm
	Suited to batch processing- Data access has high thoroughput rather than low latency
	Supports very large data sets
	
HDFS DATA
	Data is stored in multiple disks
	Each disk is on a different machine in a cluster
	NAME NODE
		-In one of this machine it randomely chooses a master node
		-On the master node hdfs runs a master process that recieves all the requests that is recieved on the  cluster.
		-Contains the mappings where the block of the file lives within the cluster
		-Replication details are also stored in name node.
		table of content
	
	DATA NODE
		All other nodes are designated as data nodes
STORING FILES IN HDFS
	* Breaks the large files into blocks
	* All these blocks are of the same size
	* For replication factor we do keep the copies of the blocks
	* The blocks are of 128 MB of size 

TIME TAKEN TO READ FROM THE BLOCK
	* Time taken to seek to the block on the disk
	* Time taken to read fron this block
	It has 2 steps
	1) Use metadata in the name node to look up block locations.
	2) Read the blocks from respective locations
CHALLENGES OF DISTRIBUTED STORAGE
	1) Failure management in data node
	2) Failure management in name node

	Note:One block in node is corrupted resulting in corrupting all the blocks in the node.

RACKS
	* Machines in a data center are stored in racks
	* Machines on single rack are close to each other and have very high bandwidth connections between them.
	* Cluster is made up of machines that are stored in different racks	
	* Nodes on different racks are away from each other than nodes on teh same rack.
	* Replicas should be stored far away from each other in different racks
	* 3 is the defaulut replication factor in hadoop
	Note: If the replicas is far from each other then the write operations would be slow
	
	STRATEGY
		1) First node is randomly chosen .Any rack that is closest to the rack would be chosen
		2) The second location would be placed in different location.
		3) Third replica would be found on the same rack but on a different node.
		"Chooses 2 racks and 3 nodes.This reduces inter rack traffic and improves write performance."
		
	Note:When a client requests would be directed to the closest node.
		
FAILURE MANAGEMENT OF NAME NODE 
	BLOCK CACHING
		Block locations are stored in memory for quick look.
	When the node is restarted each data node sends it block location to the name node.
	Failure of the data node affects the data that is stored on that node.However due replica this is not a major issue.
	Entire cluster would be completely lost without the name node.
	Name node failures are mitigated against in 2 different ways
		1)Metadata Files
			fsimage, edits store the filesystem meta data.
			fsimage:
				Complete snapshot of the file system at start up.
			edits
				Contains log information of the files across hdfs since the hadoop cluster has started.
			Both these files has default backup location
			This back up can be on a remote disk  
			Note:Merging these 2 files is very compute heavy
		2)Secondary name node
			Exact backup of the original name node.
			By checkpointing the secondary name node and name node is made sure that they are in sink.
			
MAP REDUCE
	Datset would be partitioned across the data nodes.
	MAP
		Map process work on the data that live on the same machine.
		Operations performed in parallel on small portions of the dataset.
		Code that we write for the map should process only one record.
		One record --> key value pairs 	
		A step that can be performed in parallel.
			
	REDUCE
		Output generated by the map phase would be collated together, transfered across the network within the cluster to one node, where the reduce phase runs.
		Collates all the key value pairs and combines the results in meaningful way.
		Takes all the key values -> sums,averages it -> To get the final output.
		A step to combine the intermediate results.
		
YET AGAIN RESOURCE NEGOTIATOR
	* Co-ordinates tasks running on the cluster
	* Co-ordinating all tasks on all machines in the cluster.
	* Assigns new nodes in case of failure.
	MADE OUT OF TWO COMPONENTS.
		*Deamons that run on  different machines in the cluster.
		RESOURCE MANAGER
			* Runs on the master node (Name node)
			* One instance
			* Schedule task across nodes 
			
		NODE MANAGER
			* Runs on all other nodes 
			* Multiple instance.
			* Manages tasks on the individual node
			CONTAINER
				* Run its job in it
				* Logical container inside which the process runs.
				* It is defined by resources.CPU,memory,disk
				* When a new process is requiered to be spun off on a node,the resource request is made in the form of an container.
				* The task or the process that has been assigned to this container, its responsibiliyt it is the responsibility of the container to run the task.
				* One node manager can have more than one container (Multiple process).
				* After container has been assigned the resource manager starts of the Applicaaton Master within the Container.
				* Application master is responsible to actually processing data.
				* Responsible if additional process is responsible to complete the task.
				* The application master on the original node starts off the application masters on the newly assigned nodes.
					
	LOCATION CONSTRAINT
		How does resource manager decide where to run the mapper.What nodes the mapper be run?
		Uses location constraint to locate where the mapper should be located
		Effecient 		
				
				
	
	
	
	
	
	
	 
	